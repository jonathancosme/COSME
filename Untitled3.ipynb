{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e40fdb-008d-4bc6-802d-6168e8a92772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 07:53:52.571981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 07:53:52.572426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 07:53:52.572580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from scripts.global_funcs import load_data_config\n",
    "from glob import glob\n",
    "import os\n",
    "os.environ[\"TF_MEMORY_ALLOCATION\"] = \"0.7\"  # fraction of free memory\n",
    "os.environ[\"TF_VISIBLE_DEVICE\"] = \"0\"\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader, KerasSequenceValidater\n",
    "import tensorflow as tf\n",
    "import nvtabular as nvt\n",
    "import dask_cudf\n",
    "\n",
    "# import rmm\n",
    "# from nvtabular.utils import device_mem_size\n",
    "# import shutil\n",
    "# import pathlib\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a323508f-0cca-4b7d-9a32-ad837c5cfa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configs...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading configs...\")\n",
    "configs = load_data_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de95358f-ecaf-4fb5-b97c-98fc3c76e1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting variables...\n"
     ]
    }
   ],
   "source": [
    "print(\"setting variables...\")\n",
    "output_dir = configs['output_dir']\n",
    "project_name = configs['project_name']\n",
    "input_col_name = configs['input_col_name']\n",
    "label_col_name = configs['label_col_name']\n",
    "random_seed = configs['random_seed']\n",
    "data_splits = configs['data_splits']\n",
    "base_col_names = configs['base_col_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d93b5642-226d-4580-a360-39f00b822ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_in_pathname = f\"{output_dir}/{project_name}/nvtab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14eba30-9dfa-4579-82d5-e2ff68c51ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define some information about where to get our data\n",
    "# dask_workdir = pathlib.Path(base_in_pathname, \"dask\", \"workdir\")\n",
    "# stats_path = pathlib.Path(base_in_pathname, \"dask\", \"stats\")\n",
    "\n",
    "# # Make sure we have a clean worker space for Dask\n",
    "# if pathlib.Path.is_dir(dask_workdir):\n",
    "#     shutil.rmtree(dask_workdir)\n",
    "# dask_workdir.mkdir(parents=True)\n",
    "\n",
    "# # Make sure we have a clean stats space for Dask\n",
    "# if pathlib.Path.is_dir(stats_path):\n",
    "#     shutil.rmtree(stats_path)\n",
    "# stats_path.mkdir(parents=True)\n",
    "\n",
    "# # Get device memory capacity\n",
    "# capacity = device_mem_size(kind=\"total\")\n",
    "\n",
    "# # Deploy a Single-Machine Multi-GPU Cluster\n",
    "# protocol = \"tcp\"  # \"tcp\" or \"ucx\"\n",
    "# visible_devices = \"0\"  # Delect devices to place workers\n",
    "# device_spill_frac = 0.2  # Spill GPU-Worker memory to host at this limit.\n",
    "# # Reduce if spilling fails to prevent\n",
    "# # device memory errors.\n",
    "# cluster = None  # (Optional) Specify existing scheduler port\n",
    "# if cluster is None:\n",
    "#     cluster = LocalCUDACluster(\n",
    "#         protocol=protocol,\n",
    "#         CUDA_VISIBLE_DEVICES=visible_devices,\n",
    "#         local_directory=dask_workdir,\n",
    "#         device_memory_limit=capacity * device_spill_frac,\n",
    "#         rmm_pool_size='1GB'\n",
    "#     )\n",
    "\n",
    "# # Create the distributed client\n",
    "# client = Client(cluster)\n",
    "# client\n",
    "\n",
    "# # # Initialize RMM pool on ALL workers\n",
    "# # def _rmm_pool():\n",
    "# #     rmm.reinitialize(\n",
    "# #         pool_allocator=True,\n",
    "# #         initial_pool_size=None,  # Use default size\n",
    "# #     )\n",
    "\n",
    "\n",
    "# # client.run(_rmm_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5fd082c-83f7-4d09-8ca3-de1f6d6d49c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:45209</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>1</li>\n",
       "  <li><b>Memory: </b>31.21 GiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:45209' processes=1 threads=1, memory=31.21 GiB>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a509d351-c6ec-453a-9765-aa415d43b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATHS = sorted(glob(f\"{base_in_pathname}/train/*.parquet\"))\n",
    "VAL_PATHS = sorted(glob(f\"{base_in_pathname}/val/*.parquet\"))\n",
    "TEST_PATHS = sorted(glob(f\"{base_in_pathname}/test/*.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db0fe680-190a-4aff-bc71-db6a0f4b1688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/jcosme/Data/full_mer_1/nvtab/test/part_0.parquet']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a08b109e-811f-4668-9bca-cc781f116d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16384\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c80f542-9be7-48af-b584-119049ace87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow = nvt.Workflow.load(f\"{base_in_pathname}/workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f85dc9d5-ae55-4096-a1a7-7f521311ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask_cudf.read_parquet(f\"{base_in_pathname}/val\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e88c65ab-91eb-4455-ab82-45484aa0fbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcosme/miniconda3/envs/tf/lib/python3.7/site-packages/merlin/io/parquet.py:323: UserWarning: Row group memory size (132660004) (bytes) of parquet file is bigger than requested part_size (100000000) for the NVTabular dataset.A row group memory size of 128 MB is generally recommended. You can find info on how to set the row group size of parquet files in https://nvidia-merlin.github.io/NVTabular/main/resources/troubleshooting.html#setting-the-row-group-size-for-the-parquet-files\n",
      "  f\"Row group memory size ({rg_byte_size_0}) (bytes) of parquet file is bigger\"\n"
     ]
    }
   ],
   "source": [
    "# feed them to our datasets\n",
    "train_dataset = KerasSequenceLoader(\n",
    "    nvt.Dataset(TRAIN_PATHS, part_size=\"100MB\"), # you could also use a glob pattern\n",
    "    # TRAIN_PATHS,\n",
    "    # cat_names=[input_col_name],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_names=[label_col_name],\n",
    "    shuffle=True,\n",
    "    buffer_size=0.001,  # amount of data, as a fraction of GPU memory, to load at once,\n",
    "    device=0,\n",
    "    parts_per_chunk=1,\n",
    "    engine=\"parquet\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaef00a7-6ba6-47a7-91bf-aceba802d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = KerasSequenceLoader(\n",
    "    nvt.Dataset(VAL_PATHS, part_size=\"100MB\"),   # you could also use a glob pattern\n",
    "    # VAL_PATHS,\n",
    "    # workflow.transform(cur_dataset),\n",
    "    # cat_names=[input_col_name],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_names=[label_col_name],\n",
    "    shuffle=False,\n",
    "    buffer_size=0.001,  # amount of data, as a fraction of GPU memory, to load at once,\n",
    "    device=0,\n",
    "    parts_per_chunk=1,\n",
    "    engine=\"parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f8fb3ab-861c-4188-8178-03f039c25bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = KerasSequenceLoader(\n",
    "#     nvt.Dataset(TEST_PATHS, part_size=\"100MB\"),   # you could also use a glob pattern\n",
    "#     # TEST_PATHS,\n",
    "#     # cat_names=[input_col_name],\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     label_names=[label_col_name],\n",
    "#     shuffle=False,\n",
    "#     buffer_size=0.06,  # amount of data, as a fraction of GPU memory, to load at once\n",
    "#     engine=\"parquet\",\n",
    "#     parts_per_chunk=1,\n",
    "#     device=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25733d13-51d1-4a83-aae3-dabf19605612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec93760b-2919-46b1-b7f6-0fb797f5a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f8eabcd-c577-44a3-8550-7ffd46473ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d796a050-ae71-4a88-a61d-ff41449796df",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {}\n",
    "inputs[input_col_name] = \\\n",
    "    (tf.keras.Input(name=f\"{input_col_name}__values\", dtype=tf.int64, shape=(1,)),\n",
    "     tf.keras.Input(name=f\"{input_col_name}__nnzs\", dtype=tf.int32, shape=(1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2c1c083-dde2-436f-ad52-df8bc0eecf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 07:54:31.919770: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-23 07:54:31.920574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 07:54:31.920744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 07:54:31.920857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 07:54:31.921104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 07:54:31.921226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 07:54:31.921347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-23 07:54:31.921428: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-05-23 07:54:31.921447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11468 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "inputs2 = tf.keras.layers.Lambda(lambda x: x['seq'][0])(inputs)\n",
    "throw_way = tf.keras.layers.Lambda(lambda x: x['seq'][1])(inputs)\n",
    "shape = [tf.shape(throw_way)[k] for k in range(2)]\n",
    "inputs2 = tf.reshape(inputs2, [shape[0], 150])\n",
    "inputs2 = tf.cast(inputs2, tf.float32)\n",
    "inputs2 = tf.expand_dims(inputs2, 0)\n",
    "inputs2 = tf.reshape(inputs2, [shape[0], 1, 150])\n",
    "inputs2 = tf.math.multiply(inputs2, 1/4)\n",
    "# inputs2 = tf.transpose(inputs2)\n",
    "\n",
    "# inputs2 = tf.keras.layers.Lambda(lambda x: tf.split(x, 32))(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b78436-f93e-4644-a3b1-f813d1d34640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d541ff57-3189-4152-9033-e1ddd7e65c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = pd.read_csv(f\"{output_dir}/{project_name}/data/unq_labels.csv\" ).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "516c9475-c6f8-4406-9f3b-95b164a60eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_layer = tf.keras.layers.Input(shape=(1,150,), batch_size= dtype=tf.float32)\n",
    "\n",
    "preblock_a = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='SAME')(inputs2)\n",
    "preblock_a = tf.keras.layers.BatchNormalization()(preblock_a)\n",
    "preblock_a = tf.keras.layers.Activation('gelu')(preblock_a)\n",
    "preblock_a = tf.keras.layers.MaxPool1D(pool_size=2, padding='SAME')(preblock_a)\n",
    "\n",
    "\n",
    "# block 1\n",
    "block_1_1_a = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='SAME')(preblock_a)\n",
    "block_1_1_a = tf.keras.layers.BatchNormalization()(block_1_1_a)\n",
    "block_1_1_a = tf.keras.layers.Activation('gelu')(block_1_1_a)\n",
    "block_1_1_a = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='SAME')(block_1_1_a)\n",
    "block_1_1_a = tf.keras.layers.BatchNormalization()(block_1_1_a)\n",
    "block_1_1_a = tf.keras.layers.Concatenate()([block_1_1_a, preblock_a])\n",
    "block_1_1_a = tf.keras.layers.Activation('gelu')(block_1_1_a)\n",
    "\n",
    "block_1_2_a = tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='SAME')(block_1_1_a)\n",
    "block_1_2_a = tf.keras.layers.BatchNormalization()(block_1_2_a)\n",
    "block_1_2_a = tf.keras.layers.Activation('gelu')(block_1_2_a)\n",
    "block_1_2_a = tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='SAME')(block_1_2_a)\n",
    "block_1_2_a = tf.keras.layers.BatchNormalization()(block_1_2_a)\n",
    "block_1_2_a = tf.keras.layers.Concatenate()([block_1_2_a, block_1_1_a])\n",
    "block_1_2_a = tf.keras.layers.Activation('gelu')(block_1_2_a)\n",
    "\n",
    "block_1_3_a = tf.keras.layers.Conv1D(filters=256, kernel_size=3, padding='SAME')(block_1_2_a)\n",
    "block_1_3_a = tf.keras.layers.BatchNormalization()(block_1_3_a)\n",
    "block_1_3_a = tf.keras.layers.Activation('gelu')(block_1_3_a)\n",
    "block_1_3_a = tf.keras.layers.Conv1D(filters=256, kernel_size=3, padding='SAME')(block_1_3_a)\n",
    "block_1_3_a = tf.keras.layers.BatchNormalization()(block_1_3_a)\n",
    "block_1_3_a = tf.keras.layers.Concatenate()([block_1_3_a, block_1_2_a])\n",
    "block_1_3_a = tf.keras.layers.Activation('gelu')(block_1_3_a)\n",
    "\n",
    "block_1_4_a = tf.keras.layers.Conv1D(filters=512, kernel_size=3, padding='SAME')(block_1_3_a)\n",
    "block_1_4_a = tf.keras.layers.BatchNormalization()(block_1_4_a)\n",
    "block_1_4_a = tf.keras.layers.Activation('gelu')(block_1_4_a)\n",
    "block_1_4_a = tf.keras.layers.Conv1D(filters=512, kernel_size=3, padding='SAME')(block_1_4_a)\n",
    "block_1_4_a = tf.keras.layers.BatchNormalization()(block_1_4_a)\n",
    "block_1_4_a = tf.keras.layers.Concatenate()([block_1_4_a, block_1_3_a])\n",
    "block_1_4_a = tf.keras.layers.Activation('gelu')(block_1_4_a)\n",
    "\n",
    "block_1_4_a = tf.keras.layers.AveragePooling1D(pool_size=2, padding='SAME')(block_1_4_a)\n",
    "\n",
    "\n",
    "# block 2\n",
    "block_2_1_a = tf.keras.layers.Conv1D(filters=64, kernel_size=5, padding='SAME')(preblock_a)\n",
    "block_2_1_a = tf.keras.layers.BatchNormalization()(block_2_1_a)\n",
    "block_2_1_a = tf.keras.layers.Activation('gelu')(block_2_1_a)\n",
    "block_2_1_a = tf.keras.layers.Conv1D(filters=64, kernel_size=5, padding='SAME')(block_2_1_a)\n",
    "block_2_1_a = tf.keras.layers.BatchNormalization()(block_2_1_a)\n",
    "block_2_1_a = tf.keras.layers.Concatenate()([block_2_1_a, preblock_a])\n",
    "block_2_1_a = tf.keras.layers.Activation('gelu')(block_2_1_a)\n",
    "\n",
    "block_2_2_a = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='SAME')(block_2_1_a)\n",
    "block_2_2_a = tf.keras.layers.BatchNormalization()(block_2_2_a)\n",
    "block_2_2_a = tf.keras.layers.Activation('gelu')(block_2_2_a)\n",
    "block_2_2_a = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='SAME')(block_2_2_a)\n",
    "block_2_2_a = tf.keras.layers.BatchNormalization()(block_2_2_a)\n",
    "block_2_2_a = tf.keras.layers.Concatenate()([block_2_2_a, block_2_1_a])\n",
    "block_2_2_a = tf.keras.layers.Activation('gelu')(block_2_2_a)\n",
    "\n",
    "block_2_3_a = tf.keras.layers.Conv1D(filters=256, kernel_size=5, padding='SAME')(block_2_2_a)\n",
    "block_2_3_a = tf.keras.layers.BatchNormalization()(block_2_3_a)\n",
    "block_2_3_a = tf.keras.layers.Activation('gelu')(block_2_3_a)\n",
    "block_2_3_a = tf.keras.layers.Conv1D(filters=256, kernel_size=5, padding='SAME')(block_2_3_a)\n",
    "block_2_3_a = tf.keras.layers.BatchNormalization()(block_2_3_a)\n",
    "block_2_3_a = tf.keras.layers.Concatenate()([block_2_3_a, block_2_2_a])\n",
    "block_2_3_a = tf.keras.layers.Activation('gelu')(block_2_3_a)\n",
    "\n",
    "block_2_4_a = tf.keras.layers.Conv1D(filters=512, kernel_size=5, padding='SAME')(block_2_3_a)\n",
    "block_2_4_a = tf.keras.layers.BatchNormalization()(block_2_4_a)\n",
    "block_2_4_a = tf.keras.layers.Activation('gelu')(block_2_4_a)\n",
    "block_2_4_a = tf.keras.layers.Conv1D(filters=512, kernel_size=5, padding='SAME')(block_2_4_a)\n",
    "block_2_4_a = tf.keras.layers.BatchNormalization()(block_2_4_a)\n",
    "block_2_4_a = tf.keras.layers.Concatenate()([block_2_4_a, block_2_3_a])\n",
    "block_2_4_a = tf.keras.layers.Activation('gelu')(block_2_4_a)\n",
    "\n",
    "block_2_4_a = tf.keras.layers.AveragePooling1D(pool_size=2, padding='SAME')(block_2_4_a)\n",
    "\n",
    "# block 3\n",
    "block_3_1_a = tf.keras.layers.Conv1D(filters=64, kernel_size=7, padding='SAME')(preblock_a)\n",
    "block_3_1_a = tf.keras.layers.BatchNormalization()(block_3_1_a)\n",
    "block_3_1_a = tf.keras.layers.Activation('gelu')(block_3_1_a)\n",
    "block_3_1_a = tf.keras.layers.Conv1D(filters=64, kernel_size=7, padding='SAME')(block_3_1_a)\n",
    "block_3_1_a = tf.keras.layers.BatchNormalization()(block_3_1_a)\n",
    "block_3_1_a = tf.keras.layers.Concatenate()([block_3_1_a, preblock_a])\n",
    "block_3_1_a = tf.keras.layers.Activation('gelu')(block_3_1_a)\n",
    "\n",
    "block_3_2_a = tf.keras.layers.Conv1D(filters=128, kernel_size=7, padding='SAME')(block_3_1_a)\n",
    "block_3_2_a = tf.keras.layers.BatchNormalization()(block_3_2_a)\n",
    "block_3_2_a = tf.keras.layers.Activation('gelu')(block_3_2_a)\n",
    "block_3_2_a = tf.keras.layers.Conv1D(filters=128, kernel_size=7, padding='SAME')(block_3_2_a)\n",
    "block_3_2_a = tf.keras.layers.BatchNormalization()(block_3_2_a)\n",
    "block_3_2_a = tf.keras.layers.Concatenate()([block_3_2_a, block_3_1_a])\n",
    "block_3_2_a = tf.keras.layers.Activation('gelu')(block_3_2_a)\n",
    "\n",
    "block_3_3_a = tf.keras.layers.Conv1D(filters=256, kernel_size=7, padding='SAME')(block_3_2_a)\n",
    "block_3_3_a = tf.keras.layers.BatchNormalization()(block_3_3_a)\n",
    "block_3_3_a = tf.keras.layers.Activation('gelu')(block_3_3_a)\n",
    "block_3_3_a = tf.keras.layers.Conv1D(filters=256, kernel_size=7, padding='SAME')(block_3_3_a)\n",
    "block_3_3_a = tf.keras.layers.BatchNormalization()(block_3_3_a)\n",
    "block_3_3_a = tf.keras.layers.Concatenate()([block_3_3_a, block_3_2_a])\n",
    "block_3_3_a = tf.keras.layers.Activation('gelu')(block_3_3_a)\n",
    "\n",
    "block_3_4_a = tf.keras.layers.Conv1D(filters=512, kernel_size=7, padding='SAME')(block_3_3_a)\n",
    "block_3_4_a = tf.keras.layers.BatchNormalization()(block_3_4_a)\n",
    "block_3_4_a = tf.keras.layers.Activation('gelu')(block_3_4_a)\n",
    "block_3_4_a = tf.keras.layers.Conv1D(filters=512, kernel_size=7, padding='SAME')(block_3_4_a)\n",
    "block_3_4_a = tf.keras.layers.BatchNormalization()(block_3_4_a)\n",
    "block_3_4_a = tf.keras.layers.Concatenate()([block_3_4_a, block_3_3_a])\n",
    "block_3_4_a = tf.keras.layers.Activation('gelu')(block_3_4_a)\n",
    "\n",
    "block_3_4_a = tf.keras.layers.AveragePooling1D(pool_size=2, padding='SAME')(block_3_4_a)\n",
    "\n",
    "preblock_b = tf.keras.layers.Conv1D(filters=64, kernel_size=5, padding='SAME')(inputs2)\n",
    "preblock_b = tf.keras.layers.BatchNormalization()(preblock_b)\n",
    "preblock_b = tf.keras.layers.Activation('gelu')(preblock_b)\n",
    "preblock_b = tf.keras.layers.MaxPool1D(pool_size=2, padding='SAME')(preblock_b)\n",
    "\n",
    "\n",
    "# block 1\n",
    "block_1_1_b = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='SAME')(preblock_b)\n",
    "block_1_1_b = tf.keras.layers.BatchNormalization()(block_1_1_b)\n",
    "block_1_1_b = tf.keras.layers.Activation('gelu')(block_1_1_b)\n",
    "block_1_1_b = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='SAME')(block_1_1_b)\n",
    "block_1_1_b = tf.keras.layers.BatchNormalization()(block_1_1_b)\n",
    "block_1_1_b = tf.keras.layers.Concatenate()([block_1_1_b, preblock_b])\n",
    "block_1_1_b = tf.keras.layers.Activation('gelu')(block_1_1_b)\n",
    "\n",
    "block_1_2_b = tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='SAME')(block_1_1_b)\n",
    "block_1_2_b = tf.keras.layers.BatchNormalization()(block_1_2_b)\n",
    "block_1_2_b = tf.keras.layers.Activation('gelu')(block_1_2_b)\n",
    "block_1_2_b = tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='SAME')(block_1_2_b)\n",
    "block_1_2_b = tf.keras.layers.BatchNormalization()(block_1_2_b)\n",
    "block_1_2_b = tf.keras.layers.Concatenate()([block_1_2_b, block_1_1_b])\n",
    "block_1_2_b = tf.keras.layers.Activation('gelu')(block_1_2_b)\n",
    "\n",
    "block_1_3_b = tf.keras.layers.Conv1D(filters=256, kernel_size=3, padding='SAME')(block_1_2_b)\n",
    "block_1_3_b = tf.keras.layers.BatchNormalization()(block_1_3_b)\n",
    "block_1_3_b = tf.keras.layers.Activation('gelu')(block_1_3_b)\n",
    "block_1_3_b = tf.keras.layers.Conv1D(filters=256, kernel_size=3, padding='SAME')(block_1_3_b)\n",
    "block_1_3_b = tf.keras.layers.BatchNormalization()(block_1_3_b)\n",
    "block_1_3_b = tf.keras.layers.Concatenate()([block_1_3_b, block_1_2_b])\n",
    "block_1_3_b = tf.keras.layers.Activation('gelu')(block_1_3_b)\n",
    "\n",
    "block_1_4_b = tf.keras.layers.Conv1D(filters=512, kernel_size=3, padding='SAME')(block_1_3_b)\n",
    "block_1_4_b = tf.keras.layers.BatchNormalization()(block_1_4_b)\n",
    "block_1_4_b = tf.keras.layers.Activation('gelu')(block_1_4_b)\n",
    "block_1_4_b = tf.keras.layers.Conv1D(filters=512, kernel_size=3, padding='SAME')(block_1_4_b)\n",
    "block_1_4_b = tf.keras.layers.BatchNormalization()(block_1_4_b)\n",
    "block_1_4_b = tf.keras.layers.Concatenate()([block_1_4_b, block_1_3_b])\n",
    "block_1_4_b = tf.keras.layers.Activation('gelu')(block_1_4_b)\n",
    "\n",
    "block_1_4_b = tf.keras.layers.AveragePooling1D(pool_size=2, padding='SAME')(block_1_4_b)\n",
    "\n",
    "\n",
    "# block 2\n",
    "block_2_1_b = tf.keras.layers.Conv1D(filters=64, kernel_size=5, padding='SAME')(preblock_b)\n",
    "block_2_1_b = tf.keras.layers.BatchNormalization()(block_2_1_b)\n",
    "block_2_1_b = tf.keras.layers.Activation('gelu')(block_2_1_b)\n",
    "block_2_1_b = tf.keras.layers.Conv1D(filters=64, kernel_size=5, padding='SAME')(block_2_1_b)\n",
    "block_2_1_b = tf.keras.layers.BatchNormalization()(block_2_1_b)\n",
    "block_2_1_b = tf.keras.layers.Concatenate()([block_2_1_b, preblock_b])\n",
    "block_2_1_b = tf.keras.layers.Activation('gelu')(block_2_1_b)\n",
    "\n",
    "block_2_2_b = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='SAME')(block_2_1_b)\n",
    "block_2_2_b = tf.keras.layers.BatchNormalization()(block_2_2_b)\n",
    "block_2_2_b = tf.keras.layers.Activation('gelu')(block_2_2_b)\n",
    "block_2_2_b = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='SAME')(block_2_2_b)\n",
    "block_2_2_b = tf.keras.layers.BatchNormalization()(block_2_2_b)\n",
    "block_2_2_b = tf.keras.layers.Concatenate()([block_2_2_b, block_2_1_b])\n",
    "block_2_2_b = tf.keras.layers.Activation('gelu')(block_2_2_b)\n",
    "\n",
    "block_2_3_b = tf.keras.layers.Conv1D(filters=256, kernel_size=5, padding='SAME')(block_2_2_b)\n",
    "block_2_3_b = tf.keras.layers.BatchNormalization()(block_2_3_b)\n",
    "block_2_3_b = tf.keras.layers.Activation('gelu')(block_2_3_b)\n",
    "block_2_3_b = tf.keras.layers.Conv1D(filters=256, kernel_size=5, padding='SAME')(block_2_3_b)\n",
    "block_2_3_b = tf.keras.layers.BatchNormalization()(block_2_3_b)\n",
    "block_2_3_b = tf.keras.layers.Concatenate()([block_2_3_b, block_2_2_b])\n",
    "block_2_3_b = tf.keras.layers.Activation('gelu')(block_2_3_b)\n",
    "\n",
    "block_2_4_b = tf.keras.layers.Conv1D(filters=512, kernel_size=5, padding='SAME')(block_2_3_b)\n",
    "block_2_4_b = tf.keras.layers.BatchNormalization()(block_2_4_b)\n",
    "block_2_4_b = tf.keras.layers.Activation('gelu')(block_2_4_b)\n",
    "block_2_4_b = tf.keras.layers.Conv1D(filters=512, kernel_size=5, padding='SAME')(block_2_4_b)\n",
    "block_2_4_b = tf.keras.layers.BatchNormalization()(block_2_4_b)\n",
    "block_2_4_b = tf.keras.layers.Concatenate()([block_2_4_b, block_2_3_b])\n",
    "block_2_4_b = tf.keras.layers.Activation('gelu')(block_2_4_b)\n",
    "\n",
    "block_2_4_b = tf.keras.layers.AveragePooling1D(pool_size=2, padding='SAME')(block_2_4_b)\n",
    "\n",
    "# block 3\n",
    "block_3_1_b = tf.keras.layers.Conv1D(filters=64, kernel_size=7, padding='SAME')(preblock_b)\n",
    "block_3_1_b = tf.keras.layers.BatchNormalization()(block_3_1_b)\n",
    "block_3_1_b = tf.keras.layers.Activation('gelu')(block_3_1_b)\n",
    "block_3_1_b = tf.keras.layers.Conv1D(filters=64, kernel_size=7, padding='SAME')(block_3_1_b)\n",
    "block_3_1_b = tf.keras.layers.BatchNormalization()(block_3_1_b)\n",
    "block_3_1_b = tf.keras.layers.Concatenate()([block_3_1_b, preblock_b])\n",
    "block_3_1_b = tf.keras.layers.Activation('gelu')(block_3_1_b)\n",
    "\n",
    "block_3_2_b = tf.keras.layers.Conv1D(filters=128, kernel_size=7, padding='SAME')(block_3_1_b)\n",
    "block_3_2_b = tf.keras.layers.BatchNormalization()(block_3_2_b)\n",
    "block_3_2_b = tf.keras.layers.Activation('gelu')(block_3_2_b)\n",
    "block_3_2_b = tf.keras.layers.Conv1D(filters=128, kernel_size=7, padding='SAME')(block_3_2_b)\n",
    "block_3_2_b = tf.keras.layers.BatchNormalization()(block_3_2_b)\n",
    "block_3_2_b = tf.keras.layers.Concatenate()([block_3_2_b, block_3_1_b])\n",
    "block_3_2_b = tf.keras.layers.Activation('gelu')(block_3_2_b)\n",
    "\n",
    "block_3_3_b = tf.keras.layers.Conv1D(filters=256, kernel_size=7, padding='SAME')(block_3_2_b)\n",
    "block_3_3_b = tf.keras.layers.BatchNormalization()(block_3_3_b)\n",
    "block_3_3_b = tf.keras.layers.Activation('gelu')(block_3_3_b)\n",
    "block_3_3_b = tf.keras.layers.Conv1D(filters=256, kernel_size=7, padding='SAME')(block_3_3_b)\n",
    "block_3_3_b = tf.keras.layers.BatchNormalization()(block_3_3_b)\n",
    "block_3_3_b = tf.keras.layers.Concatenate()([block_3_3_b, block_3_2_b])\n",
    "block_3_3_b = tf.keras.layers.Activation('gelu')(block_3_3_b)\n",
    "\n",
    "block_3_4_b = tf.keras.layers.Conv1D(filters=512, kernel_size=7, padding='SAME')(block_3_3_b)\n",
    "block_3_4_b = tf.keras.layers.BatchNormalization()(block_3_4_b)\n",
    "block_3_4_b = tf.keras.layers.Activation('gelu')(block_3_4_b)\n",
    "block_3_4_b = tf.keras.layers.Conv1D(filters=512, kernel_size=7, padding='SAME')(block_3_4_b)\n",
    "block_3_4_b = tf.keras.layers.BatchNormalization()(block_3_4_b)\n",
    "block_3_4_b = tf.keras.layers.Concatenate()([block_3_4_b, block_3_3_b])\n",
    "block_3_4_b = tf.keras.layers.Activation('gelu')(block_3_4_b)\n",
    "\n",
    "block_3_4_b = tf.keras.layers.AveragePooling1D(pool_size=2, padding='SAME')(block_3_4_b)\n",
    "\n",
    "preblock_c = tf.keras.layers.Conv1D(filters=64, kernel_size=7, padding='SAME')(inputs2)\n",
    "preblock_c = tf.keras.layers.BatchNormalization()(preblock_c)\n",
    "preblock_c = tf.keras.layers.Activation('gelu')(preblock_c)\n",
    "preblock_c = tf.keras.layers.MaxPool1D(pool_size=2, padding='SAME')(preblock_c)\n",
    "\n",
    "\n",
    "# block 1\n",
    "block_1_1_c = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='SAME')(preblock_c)\n",
    "block_1_1_c = tf.keras.layers.BatchNormalization()(block_1_1_c)\n",
    "block_1_1_c = tf.keras.layers.Activation('gelu')(block_1_1_c)\n",
    "block_1_1_c = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='SAME')(block_1_1_c)\n",
    "block_1_1_c = tf.keras.layers.BatchNormalization()(block_1_1_c)\n",
    "block_1_1_c = tf.keras.layers.Concatenate()([block_1_1_c, preblock_c])\n",
    "block_1_1_c = tf.keras.layers.Activation('gelu')(block_1_1_c)\n",
    "\n",
    "block_1_2_c = tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='SAME')(block_1_1_c)\n",
    "block_1_2_c = tf.keras.layers.BatchNormalization()(block_1_2_c)\n",
    "block_1_2_c = tf.keras.layers.Activation('gelu')(block_1_2_c)\n",
    "block_1_2_c = tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='SAME')(block_1_2_c)\n",
    "block_1_2_c = tf.keras.layers.BatchNormalization()(block_1_2_c)\n",
    "block_1_2_c = tf.keras.layers.Concatenate()([block_1_2_c, block_1_1_c])\n",
    "block_1_2_c = tf.keras.layers.Activation('gelu')(block_1_2_c)\n",
    "\n",
    "block_1_3_c = tf.keras.layers.Conv1D(filters=256, kernel_size=3, padding='SAME')(block_1_2_c)\n",
    "block_1_3_c = tf.keras.layers.BatchNormalization()(block_1_3_c)\n",
    "block_1_3_c = tf.keras.layers.Activation('gelu')(block_1_3_c)\n",
    "block_1_3_c = tf.keras.layers.Conv1D(filters=256, kernel_size=3, padding='SAME')(block_1_3_c)\n",
    "block_1_3_c = tf.keras.layers.BatchNormalization()(block_1_3_c)\n",
    "block_1_3_c = tf.keras.layers.Concatenate()([block_1_3_c, block_1_2_c])\n",
    "block_1_3_c = tf.keras.layers.Activation('gelu')(block_1_3_c)\n",
    "\n",
    "block_1_4_c = tf.keras.layers.Conv1D(filters=512, kernel_size=3, padding='SAME')(block_1_3_c)\n",
    "block_1_4_c = tf.keras.layers.BatchNormalization()(block_1_4_c)\n",
    "block_1_4_c = tf.keras.layers.Activation('gelu')(block_1_4_c)\n",
    "block_1_4_c = tf.keras.layers.Conv1D(filters=512, kernel_size=3, padding='SAME')(block_1_4_c)\n",
    "block_1_4_c = tf.keras.layers.BatchNormalization()(block_1_4_c)\n",
    "block_1_4_c = tf.keras.layers.Concatenate()([block_1_4_c, block_1_3_c])\n",
    "block_1_4_c = tf.keras.layers.Activation('gelu')(block_1_4_c)\n",
    "\n",
    "block_1_4_c = tf.keras.layers.AveragePooling1D(pool_size=2, padding='SAME')(block_1_4_c)\n",
    "\n",
    "\n",
    "# block 2\n",
    "block_2_1_c = tf.keras.layers.Conv1D(filters=64, kernel_size=5, padding='SAME')(preblock_c)\n",
    "block_2_1_c = tf.keras.layers.BatchNormalization()(block_2_1_c)\n",
    "block_2_1_c = tf.keras.layers.Activation('gelu')(block_2_1_c)\n",
    "block_2_1_c = tf.keras.layers.Conv1D(filters=64, kernel_size=5, padding='SAME')(block_2_1_c)\n",
    "block_2_1_c = tf.keras.layers.BatchNormalization()(block_2_1_c)\n",
    "block_2_1_c = tf.keras.layers.Concatenate()([block_2_1_c, preblock_c])\n",
    "block_2_1_c = tf.keras.layers.Activation('gelu')(block_2_1_c)\n",
    "\n",
    "block_2_2_c = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='SAME')(block_2_1_c)\n",
    "block_2_2_c = tf.keras.layers.BatchNormalization()(block_2_2_c)\n",
    "block_2_2_c = tf.keras.layers.Activation('gelu')(block_2_2_c)\n",
    "block_2_2_c = tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='SAME')(block_2_2_c)\n",
    "block_2_2_c = tf.keras.layers.BatchNormalization()(block_2_2_c)\n",
    "block_2_2_c = tf.keras.layers.Concatenate()([block_2_2_c, block_2_1_c])\n",
    "block_2_2_c = tf.keras.layers.Activation('gelu')(block_2_2_c)\n",
    "\n",
    "block_2_3_c = tf.keras.layers.Conv1D(filters=256, kernel_size=5, padding='SAME')(block_2_2_c)\n",
    "block_2_3_c = tf.keras.layers.BatchNormalization()(block_2_3_c)\n",
    "block_2_3_c = tf.keras.layers.Activation('gelu')(block_2_3_c)\n",
    "block_2_3_c = tf.keras.layers.Conv1D(filters=256, kernel_size=5, padding='SAME')(block_2_3_c)\n",
    "block_2_3_c = tf.keras.layers.BatchNormalization()(block_2_3_c)\n",
    "block_2_3_c = tf.keras.layers.Concatenate()([block_2_3_c, block_2_2_c])\n",
    "block_2_3_c = tf.keras.layers.Activation('gelu')(block_2_3_c)\n",
    "\n",
    "block_2_4_c = tf.keras.layers.Conv1D(filters=512, kernel_size=5, padding='SAME')(block_2_3_c)\n",
    "block_2_4_c = tf.keras.layers.BatchNormalization()(block_2_4_c)\n",
    "block_2_4_c = tf.keras.layers.Activation('gelu')(block_2_4_c)\n",
    "block_2_4_c = tf.keras.layers.Conv1D(filters=512, kernel_size=5, padding='SAME')(block_2_4_c)\n",
    "block_2_4_c = tf.keras.layers.BatchNormalization()(block_2_4_c)\n",
    "block_2_4_c = tf.keras.layers.Concatenate()([block_2_4_c, block_2_3_c])\n",
    "block_2_4_c = tf.keras.layers.Activation('gelu')(block_2_4_c)\n",
    "\n",
    "block_2_4_c = tf.keras.layers.AveragePooling1D(pool_size=2, padding='SAME')(block_2_4_c)\n",
    "\n",
    "# block 3\n",
    "block_3_1_c = tf.keras.layers.Conv1D(filters=64, kernel_size=7, padding='SAME')(preblock_c)\n",
    "block_3_1_c = tf.keras.layers.BatchNormalization()(block_3_1_c)\n",
    "block_3_1_c = tf.keras.layers.Activation('gelu')(block_3_1_c)\n",
    "block_3_1_c = tf.keras.layers.Conv1D(filters=64, kernel_size=7, padding='SAME')(block_3_1_c)\n",
    "block_3_1_c = tf.keras.layers.BatchNormalization()(block_3_1_c)\n",
    "block_3_1_c = tf.keras.layers.Concatenate()([block_3_1_c, preblock_c])\n",
    "block_3_1_c = tf.keras.layers.Activation('gelu')(block_3_1_c)\n",
    "\n",
    "block_3_2_c = tf.keras.layers.Conv1D(filters=128, kernel_size=7, padding='SAME')(block_3_1_c)\n",
    "block_3_2_c = tf.keras.layers.BatchNormalization()(block_3_2_c)\n",
    "block_3_2_c = tf.keras.layers.Activation('gelu')(block_3_2_c)\n",
    "block_3_2_c = tf.keras.layers.Conv1D(filters=128, kernel_size=7, padding='SAME')(block_3_2_c)\n",
    "block_3_2_c = tf.keras.layers.BatchNormalization()(block_3_2_c)\n",
    "block_3_2_c = tf.keras.layers.Concatenate()([block_3_2_c, block_3_1_c])\n",
    "block_3_2_c = tf.keras.layers.Activation('gelu')(block_3_2_c)\n",
    "\n",
    "block_3_3_c = tf.keras.layers.Conv1D(filters=256, kernel_size=7, padding='SAME')(block_3_2_c)\n",
    "block_3_3_c = tf.keras.layers.BatchNormalization()(block_3_3_c)\n",
    "block_3_3_c = tf.keras.layers.Activation('gelu')(block_3_3_c)\n",
    "block_3_3_c = tf.keras.layers.Conv1D(filters=256, kernel_size=7, padding='SAME')(block_3_3_c)\n",
    "block_3_3_c = tf.keras.layers.BatchNormalization()(block_3_3_c)\n",
    "block_3_3_c = tf.keras.layers.Concatenate()([block_3_3_c, block_3_2_c])\n",
    "block_3_3_c = tf.keras.layers.Activation('gelu')(block_3_3_c)\n",
    "\n",
    "block_3_4_c = tf.keras.layers.Conv1D(filters=512, kernel_size=7, padding='SAME')(block_3_3_c)\n",
    "block_3_4_c = tf.keras.layers.BatchNormalization()(block_3_4_c)\n",
    "block_3_4_c = tf.keras.layers.Activation('gelu')(block_3_4_c)\n",
    "block_3_4_c = tf.keras.layers.Conv1D(filters=512, kernel_size=7, padding='SAME')(block_3_4_c)\n",
    "block_3_4_c = tf.keras.layers.BatchNormalization()(block_3_4_c)\n",
    "block_3_4_c = tf.keras.layers.Concatenate()([block_3_4_c, block_3_3_c])\n",
    "block_3_4_c = tf.keras.layers.Activation('gelu')(block_3_4_c)\n",
    "\n",
    "block_3_4_c = tf.keras.layers.AveragePooling1D(pool_size=2, padding='SAME')(block_3_4_c)\n",
    "\n",
    "output_layer = tf.keras.layers.Concatenate()([block_1_4_a, block_2_4_a, block_3_4_a, block_1_4_b, block_2_4_b, block_3_4_b, block_1_4_c, block_2_4_c, block_3_4_c])\n",
    "\n",
    "# output_layer = tf.keras.layers.Concatenate()([block_1_4_a, block_2_4_a, block_3_4_a,])\n",
    "output_layer = tf.keras.layers.Flatten()(output_layer)\n",
    "output_layer = tf.keras.layers.Dense(n_classes)(output_layer)\n",
    "output_layer = tf.keras.layers.Activation('tanh')(output_layer)\n",
    "# output_layer = tf.keras.layers.Dropout(0.8)(output_layer)\n",
    "output_layer = tf.keras.layers.Dense(n_classes)(output_layer)\n",
    "output_layer = tf.keras.layers.LeakyReLU(0.2)(output_layer)\n",
    "# output_layer = tf.keras.layers.Dropout(0.8)(output_layer)\n",
    "output_layer = tf.keras.layers.Dense(n_classes)(output_layer)\n",
    "output_layer = tf.keras.layers.Activation('softmax')(output_layer)\n",
    "# output_layer = tf.keras.layers.Dropout(0.8)(output_layer)\n",
    "# output_layer = tf.keras.layers.Dense(n_classes)(output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "019100bb-960f-46a6-b930-369e127bb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c890ca85-f37e-4954-9281-1a6da3e4a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92b12af3-56df-427b-8cad-70e2897bf27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer, \n",
    "              tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "              metrics=['sparse_categorical_accuracy'],\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5f323ed-7383-467f-ad16-0078ec6e4080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2aa3bfb-3d4d-41fe-82ae-b652ef196245",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_callback = KerasSequenceValidater(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61e860a4-38c9-4264-867b-6113ca64b7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 07:55:16.486044: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8201\n",
      "2022-05-23 07:55:17.159022: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-05-23 07:55:18.815988: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35/817 [>.............................] - ETA: 9:53 - loss: 4.7590 - sparse_categorical_accuracy: 0.1795"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28262/1311933465.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# validation_data = valid_dataset,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidation_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    # validation_data = valid_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[validation_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55314bf-db6a-47fa-a493-bf76afe6c389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e7f39-0641-4832-b4a8-96975317224e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14e53c89-a383-4a1f-884f-75c6cfecbf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d92b7-f2d0-400b-a58d-4229cbd43a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4ee6ff-ceee-4530-9131-a1d219571760",
   "metadata": {},
   "source": [
    "# make nvtab data\n",
    "\n",
    "This is an explanation of the  \n",
    "**make_nvtab.py**    \n",
    "file.  \n",
    "\n",
    "We take our split data and create:\n",
    "+ NVtabublar datasets\n",
    "\n",
    "Then we will save the output as parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0ed94-a847-47f3-9b09-ff6322b53022",
   "metadata": {},
   "source": [
    "## Step 0: import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27df203-f0c2-4538-91f3-45b31f21367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') # this is to allow the script to read from the parent folder\n",
    "\n",
    "from scripts.global_funcs import load_data_config\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import dask_cudf\n",
    "import nvtabular as nvt\n",
    "\n",
    "# import rmm\n",
    "# from nvtabular.utils import device_mem_size\n",
    "# import shutil\n",
    "# import pathlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048b7bc-34b1-4298-ad93-6b906b5bc9c7",
   "metadata": {},
   "source": [
    "## Step 1: load config file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfafaeb-72cf-41e0-94c4-791f96705efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = load_data_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737c28ad-d3a7-4172-8d3c-db24119588ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_fasta_file: /media/jcosme/Data/MarRef_parquet_10_cats\n",
      "output_dir: /media/jcosme/Data\n",
      "project_name: MarRef_parquet_10_cats\n",
      "base_col_names: ['seq', 'label']\n",
      "label_col_name: label\n",
      "input_col_name: seq\n",
      "label_regex: (?:[^a-zA-Z0-9]+)([a-zA-Z]+[0-9]+)(?:[^a-zA-Z0-9]+)\n",
      "k_mer: 1\n",
      "possible_gene_values: ['A', 'C', 'G', 'T']\n",
      "max_seq_len: 150\n",
      "data_splits: {'train': 0.9, 'val': 0.05, 'test': 0.05}\n",
      "random_seed: 42\n",
      "fasta_sep: >\n",
      "unq_labs_dir: /media/jcosme/Data/MarRef_parquet_10_cats/data/unq_labels\n",
      "unq_labs_dir_csv: /media/jcosme/Data/MarRef_parquet_10_cats/data/unq_labels.csv\n",
      "data_dir: /media/jcosme/Data/MarRef_parquet_10_cats/data/MarRef_parquet_10_cats\n",
      "nvtab_dir: /media/jcosme/Data/MarRef_parquet_10_cats/nvtab\n",
      "dask_dir: /media/jcosme/Data/MarRef_parquet_10_cats/dask\n",
      "tensorboard_dir: /media/jcosme/Data/MarRef_parquet_10_cats/tensorboard\n",
      "model_checkpoints_dir: /media/jcosme/Data/MarRef_parquet_10_cats/checkpoints/model_checkpoints\n",
      "model_checkpoints_parent_dir: /media/jcosme/Data/MarRef_parquet_10_cats/checkpoints\n",
      "model_weights_dir: /media/jcosme/Data/MarRef_parquet_10_cats/model_weights.h5\n"
     ]
    }
   ],
   "source": [
    "# these are the variables we will be using\n",
    "for key, val in configs.items():\n",
    "    print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58858103-d4a3-4862-bd56-8a609d70dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets put these into python variables\n",
    "input_col_name = configs['input_col_name']\n",
    "label_col_name = configs['label_col_name']\n",
    "data_splits = configs['data_splits']\n",
    "max_seq_len = configs['max_seq_len']\n",
    "nvtab_dir = configs['nvtab_dir']\n",
    "data_dir = configs['data_dir']\n",
    "dask_dir = configs['dask_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67fa09c-9f2e-4cf9-b86f-56f80beb80bd",
   "metadata": {},
   "source": [
    "## Step 2: start the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f49ae47-945d-4191-974e-95e52117846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define some information about where to get our data\n",
    "# dask_workdir = pathlib.Path(nvtab_dir, \"dask\", \"workdir\")\n",
    "# stats_path = pathlib.Path(nvtab_dir, \"dask\", \"stats\")\n",
    "\n",
    "# # Make sure we have a clean worker space for Dask\n",
    "# if pathlib.Path.is_dir(dask_workdir):\n",
    "#     shutil.rmtree(dask_workdir)\n",
    "# dask_workdir.mkdir(parents=True)\n",
    "\n",
    "# # Make sure we have a clean stats space for Dask\n",
    "# if pathlib.Path.is_dir(stats_path):\n",
    "#     shutil.rmtree(stats_path)\n",
    "# stats_path.mkdir(parents=True)\n",
    "\n",
    "# # Get device memory capacity\n",
    "# capacity = device_mem_size(kind=\"total\")\n",
    "\n",
    "# # Deploy a Single-Machine Multi-GPU Cluster\n",
    "# protocol = \"tcp\"  # \"tcp\" or \"ucx\"\n",
    "# visible_devices = \"0\"  # Delect devices to place workers\n",
    "# device_spill_frac = 0.5  # Spill GPU-Worker memory to host at this limit.\n",
    "# # Reduce if spilling fails to prevent\n",
    "# # device memory errors.\n",
    "# cluster = None  # (Optional) Specify existing scheduler port\n",
    "# if cluster is None:\n",
    "#     cluster = LocalCUDACluster(\n",
    "#         protocol=protocol,\n",
    "#         CUDA_VISIBLE_DEVICES=visible_devices,\n",
    "#         local_directory=dask_workdir,\n",
    "#         device_memory_limit=capacity * device_spill_frac,\n",
    "#     )\n",
    "\n",
    "# # Create the distributed client\n",
    "# client = Client(cluster)\n",
    "# client\n",
    "\n",
    "# # Initialize RMM pool on ALL workers\n",
    "# def _rmm_pool():\n",
    "#     rmm.reinitialize(\n",
    "#         pool_allocator=True,\n",
    "#         initial_pool_size=None,  # Use default size\n",
    "#     )\n",
    "\n",
    "\n",
    "# client.run(_rmm_pool)\n",
    "# client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4b3f2f-91d9-4f68-83d9-916f0f42cd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:37919</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>1</li>\n",
       "  <li><b>Memory: </b>31.21 GiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:37919' processes=1 threads=1, memory=31.21 GiB>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = LocalCUDACluster(local_directory=dask_dir)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8cca65-5240-46c6-a735-6b4f720b0c7b",
   "metadata": {},
   "source": [
    "### optional: \n",
    "click the link above to open up the Dask Dashboard, which will allow you to see the progress of your job.  \n",
    "**note:** this will only work on a jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9599a1e1-358a-4f98-a35d-75214885d0ed",
   "metadata": {},
   "source": [
    "## Step 3: create NVTabular workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c34f7cd-6890-4316-8ff0-3792ab8cd122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pipeline\n",
    "# nvt.ColumnGroup(\n",
    "cat_features =  [input_col_name] >> nvt.ops.Categorify() >> nvt.ops.ListSlice(0, end=150, pad=True, pad_value=0.0)\n",
    "\n",
    "# add label column\n",
    "output = cat_features + label_col_name\n",
    "\n",
    "# create workflow\n",
    "workflow = nvt.Workflow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b8d1d-9c23-4d3c-83fe-105a1b34f27c",
   "metadata": {},
   "source": [
    "## Step 5: fit workflow on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e029e60a-220e-4158-8cff-03a658d9c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting nvtab workflow on training data...\n",
      "saving fitting nvtab workflow...\n",
      "CPU times: user 441 ms, sys: 216 ms, total: 656 ms\n",
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fitting on training data, and saving the workflow\n",
    "for key in data_splits.keys():\n",
    "    if key=='train':\n",
    "        print(\"fitting nvtab workflow on training data...\")\n",
    "        workflow.fit(nvt.Dataset(f\"{data_dir}_{key}\", engine='parquet', row_group_size=10000))\n",
    "\n",
    "        print(\"saving fitting nvtab workflow...\")\n",
    "        workflow.save(f\"{nvtab_dir}/workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc44426d-4219-4bb5-8095-e913358667e7",
   "metadata": {},
   "source": [
    "# Step 6: create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a576d9cc-19fb-4b3e-b7ec-70ae8a3ea491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making nvtab dataset for training...\n",
      "making nvtab dataset for val...\n",
      "making nvtab dataset for test...\n",
      "CPU times: user 140 ms, sys: 4.32 ms, total: 144 ms\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "shuffle= nvt.io.Shuffle.PER_PARTITION\n",
    "\n",
    "for key in data_splits.keys():\n",
    "    if key=='train':\n",
    "\n",
    "        print(\"making nvtab dataset for training...\")\n",
    "        workflow.transform(nvt.Dataset(f\"{data_dir}_{key}\", engine='parquet', row_group_size=10000)).to_parquet(\n",
    "            output_path=f\"{nvtab_dir}/{key}\",\n",
    "            shuffle=shuffle,\n",
    "            cats=[input_col_name],\n",
    "            labels=[label_col_name],\n",
    "        )\n",
    "    else:\n",
    "        print(f\"making nvtab dataset for {key}...\")\n",
    "        workflow.transform(nvt.Dataset(f\"{data_dir}_{key}\", engine='parquet', row_group_size=10000)).to_parquet(\n",
    "            output_path=f\"{nvtab_dir}/{key}\",\n",
    "            shuffle=None,\n",
    "            out_files_per_proc=None,\n",
    "            cats=[input_col_name],\n",
    "            labels=[label_col_name],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c7bf7-f545-4784-b863-f2fce40dc76b",
   "metadata": {},
   "source": [
    "## Step 7: cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb06190f-2305-42a6-b507-f8e566308f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcosme/miniconda3/envs/tf/lib/python3.7/site-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: \u001b[1mGrid size (45) < 2 * SM count (96) will likely result in GPU under utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/jcosme/miniconda3/envs/tf/lib/python3.7/site-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: \u001b[1mGrid size (77) < 2 * SM count (96) will likely result in GPU under utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/jcosme/miniconda3/envs/tf/lib/python3.7/site-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: \u001b[1mGrid size (82) < 2 * SM count (96) will likely result in GPU under utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/jcosme/miniconda3/envs/tf/lib/python3.7/site-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: \u001b[1mGrid size (46) < 2 * SM count (96) will likely result in GPU under utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/jcosme/miniconda3/envs/tf/lib/python3.7/site-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: \u001b[1mGrid size (78) < 2 * SM count (96) will likely result in GPU under utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/jcosme/miniconda3/envs/tf/lib/python3.7/site-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: \u001b[1mGrid size (47) < 2 * SM count (96) will likely result in GPU under utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# shutdown the Dask cluster\n",
    "client.shutdown()\n",
    "\n",
    "# finally we close the Dask cluster\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a95179-8111-441e-bb77-58b5622c451e",
   "metadata": {},
   "source": [
    "## finished!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c22b19-7f5c-445e-b87f-ff35c869ed0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

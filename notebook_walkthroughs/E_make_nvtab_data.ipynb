{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4ee6ff-ceee-4530-9131-a1d219571760",
   "metadata": {},
   "source": [
    "# make nvtab data\n",
    "\n",
    "This is an explanation of the  \n",
    "**make_nvtab.py**    \n",
    "file.  \n",
    "\n",
    "We take our split data and create:\n",
    "+ NVtabublar datasets\n",
    "\n",
    "Then we will save the output as parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0ed94-a847-47f3-9b09-ff6322b53022",
   "metadata": {},
   "source": [
    "## Step 0: import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27df203-f0c2-4538-91f3-45b31f21367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') # this is to allow the script to read from the parent folder\n",
    "\n",
    "from scripts.global_funcs import load_data_config\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import dask_cudf\n",
    "import nvtabular as nvt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67fa09c-9f2e-4cf9-b86f-56f80beb80bd",
   "metadata": {},
   "source": [
    "## Step 1: start the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4b3f2f-91d9-4f68-83d9-916f0f42cd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:46655</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>1</li>\n",
       "  <li><b>Memory: </b>31.21 GiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:46655' processes=1 threads=1, memory=31.21 GiB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8cca65-5240-46c6-a735-6b4f720b0c7b",
   "metadata": {},
   "source": [
    "### optional: \n",
    "click the link above to open up the Dask Dashboard, which will allow you to see the progress of your job.  \n",
    "**note:** this will only work on a jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395c4d9-4d5f-4993-9387-d98e3f97e920",
   "metadata": {},
   "source": [
    "## Step 2: load config file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfafaeb-72cf-41e0-94c4-791f96705efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = load_data_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737c28ad-d3a7-4172-8d3c-db24119588ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_fasta_file: /media/jcosme/Data/MarRef_parquet\n",
      "output_dir: /media/jcosme/Data\n",
      "project_name: full_mer_1\n",
      "base_col_names: ['seq', 'label']\n",
      "label_col_name: label\n",
      "input_col_name: seq\n",
      "label_regex: (?:[^a-zA-Z0-9]+)([a-zA-Z]+[0-9]+)(?:[^a-zA-Z0-9]+)\n",
      "k_mer: 1\n",
      "possible_gene_values: ['A', 'C', 'G', 'T']\n",
      "max_seq_len: 150\n",
      "data_splits: {'train': 0.9, 'val': 0.05, 'test': 0.05}\n",
      "random_seed: 42\n",
      "fasta_sep: >\n",
      "unq_labs_dir: /media/jcosme/Data/full_mer_1/data/unq_labels\n",
      "unq_labs_dir_csv: /media/jcosme/Data/full_mer_1/data/unq_labels.csv\n",
      "data_dir: /media/jcosme/Data/full_mer_1/data/full_mer_1\n",
      "nvtab_dir: /media/jcosme/Data/full_mer_1/nvtab\n"
     ]
    }
   ],
   "source": [
    "# these are the variables we will be using\n",
    "for key, val in configs.items():\n",
    "    print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58858103-d4a3-4862-bd56-8a609d70dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets put these into python variables\n",
    "input_col_name = configs['input_col_name']\n",
    "label_col_name = configs['label_col_name']\n",
    "data_splits = configs['data_splits']\n",
    "max_seq_len = configs['max_seq_len']\n",
    "nvtab_dir = configs['nvtab_dir']\n",
    "data_dir = configs['data_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9599a1e1-358a-4f98-a35d-75214885d0ed",
   "metadata": {},
   "source": [
    "## Step 3: create NVTabular workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c34f7cd-6890-4316-8ff0-3792ab8cd122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pipeline\n",
    "cat_features = [input_col_name] >> nvt.ops.Categorify() >>  nvt.ops.ListSlice(start=0, end=max_seq_len, pad=True, pad_value=0)\n",
    "\n",
    "# add label column\n",
    "output = cat_features + label_col_name\n",
    "\n",
    "# create workflow\n",
    "workflow = nvt.Workflow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b8d1d-9c23-4d3c-83fe-105a1b34f27c",
   "metadata": {},
   "source": [
    "## Step 5: fit workflow on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e029e60a-220e-4158-8cff-03a658d9c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting nvtab workflow on training data...\n",
      "saving fitting nvtab workflow...\n",
      "CPU times: user 801 ms, sys: 213 ms, total: 1.01 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fitting on training data, and saving the workflow\n",
    "for key in data_splits.keys():\n",
    "    if key=='train':\n",
    "        print(\"fitting nvtab workflow on training data...\")\n",
    "        workflow.fit(nvt.Dataset(f\"{data_dir}_{key}\", engine='parquet', row_group_size=10000))\n",
    "\n",
    "        print(\"saving fitting nvtab workflow...\")\n",
    "        workflow.save(f\"{nvtab_dir}/workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc44426d-4219-4bb5-8095-e913358667e7",
   "metadata": {},
   "source": [
    "# Step 6: create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a576d9cc-19fb-4b3e-b7ec-70ae8a3ea491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making nvtab dataset for training...\n",
      "making nvtab dataset for {key}...\n",
      "making nvtab dataset for {key}...\n",
      "CPU times: user 792 ms, sys: 198 ms, total: 990 ms\n",
      "Wall time: 51.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "shuffle= nvt.io.Shuffle.PER_PARTITION\n",
    "\n",
    "for key in data_splits.keys():\n",
    "    if key=='train':\n",
    "\n",
    "        print(\"making nvtab dataset for training...\")\n",
    "        workflow.transform(nvt.Dataset(f\"{data_dir}_{key}\", engine='parquet', row_group_size=10000)).to_parquet(\n",
    "            output_path=f\"{nvtab_dir}/{key}\",\n",
    "            shuffle=shuffle,\n",
    "            cats=[input_col_name],\n",
    "            labels=[label_col_name],\n",
    "        )\n",
    "    else:\n",
    "        print(\"making nvtab dataset for {key}...\")\n",
    "        workflow.transform(nvt.Dataset(f\"{data_dir}_{key}\", engine='parquet', row_group_size=10000)).to_parquet(\n",
    "            output_path=f\"{nvtab_dir}/{key}\",\n",
    "            shuffle=None,\n",
    "            out_files_per_proc=None,\n",
    "            cats=[input_col_name],\n",
    "            labels=[label_col_name],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c7bf7-f545-4784-b863-f2fce40dc76b",
   "metadata": {},
   "source": [
    "## Step 7: cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb06190f-2305-42a6-b507-f8e566308f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcosme/miniconda3/envs/tf/lib/python3.7/site-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: \u001b[1mGrid size (8) < 2 * SM count (96) will likely result in GPU under utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# shutdown the Dask cluster\n",
    "client.shutdown()\n",
    "\n",
    "# finally we close the Dask cluster\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a95179-8111-441e-bb77-58b5622c451e",
   "metadata": {},
   "source": [
    "## finished!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c22b19-7f5c-445e-b87f-ff35c869ed0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
